{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.output_parsers import GuardrailsOutputParser\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "settings = {\n",
    "    'OPENAI_API_KEY': 'YOUR_API_KEY',\n",
    "}\n",
    "\n",
    "settings = SimpleNamespace(**settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Guardrails Output Parser\n",
    "\n",
    "The prompt element in the Guardrails output parser is the prompt of the final chain in the pipeline. This is the prompt that will be used to generate the Guardrails output.\n",
    "\n",
    "In this case, this is the prompt for the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOTSONIC_TEMPLATE = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "    <output>\n",
    "        <string name=\"result\" description=\"chatbot's response\" required=\"true\"/>\n",
    "    </output>\n",
    "\n",
    "    <instructions>\n",
    "        You are a helpful bot, you are provided with matching data from the knowledge base related to the query from the user. You are to help users provide information properly.\n",
    "        \n",
    "        @json_suffix_prompt_examples\n",
    "    </instructions>\n",
    "\n",
    "    <prompt>\n",
    "    @xml_prefix_prompt\n",
    "\n",
    "    {output_schema}\n",
    "    </prompt>\n",
    "</rail>\n",
    "\"\"\"\n",
    "\n",
    "output_parser = GuardrailsOutputParser.from_rail_string(BOTSONIC_TEMPLATE, num_reasks=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ConversationalRetrieverChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Botsonic team to set up the following:\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are having a conversation with an AI assistant. The assistant is here to help you with your questions. The assistant is very knowledgeable about the topic you are discussing.\"\n",
    "MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate sub chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Generator Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=MODEL, temperature=0, openai_api_key=settings.OPENAI_API_KEY\n",
    ")\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering Chain\n",
    "\n",
    "**Note:**\n",
    "\n",
    "This is the final chain in the pipeline. It is responsible for generating the answer to the question.\n",
    "Therefore, the prompt for this chain needs to be updated in order to include Guardrails style prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "This is a test context\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `None`.\n",
      "\n",
      "Here are examples of simple (XML, JSON) pairs that show the expected behavior:\n",
      "- `<string name='foo' format='two-words lower-case' />` => `{{'foo': 'example one'}}`\n",
      "- `<list name='bar'><string format='upper-case' /></list>` => `{{\"bar\": ['STRING ONE', 'STRING TWO', etc.]}}`\n",
      "- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{{'baz': {{'foo': 'Some String', 'index': 1}}}}`\n",
      "\n",
      "    \n",
      "\n",
      "Human: \n",
      "Answer the question below, and return a JSON that follows the correct schema.\n",
      "\n",
      "This is a test question\n",
      "\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "\n",
      "    <output>\n",
      "    <string name=\"result\" description=\"chatbot's response\" required=\"true\"/>\n",
      "</output>\n",
      "\n",
      "    \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting up prompt\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "Use the following pieces of context to answer the users question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "human_msg_template = HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Answer the question below, and return a JSON that follows the correct schema.\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions_human_msg}\"\n",
    "\"\"\")\n",
    "\n",
    "CHAT_PROMPT = ChatPromptTemplate(\n",
    "    messages=[system_template, human_msg_template],\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": output_parser.guard.rail.instructions.format_instructions,\n",
    "        \"format_instructions_human_msg\": output_parser.guard.rail.prompt.format_instructions,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(CHAT_PROMPT.format(context=\"This is a test context\", question=\"This is a test question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_llm = ChatOpenAI(\n",
    "    model_name=MODEL,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    temperature=0,\n",
    "    openai_api_key=settings.OPENAI_API_KEY,\n",
    ")\n",
    "doc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=CHAT_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: \\nUse the following pieces of context to answer the users question. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n----------------\\nThis is a test context\\n\\n\\nONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML\\'s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `None`.\\n\\nHere are examples of simple (XML, JSON) pairs that show the expected behavior:\\n- `<string name=\\'foo\\' format=\\'two-words lower-case\\' />` => `{{\\'foo\\': \\'example one\\'}}`\\n- `<list name=\\'bar\\'><string format=\\'upper-case\\' /></list>` => `{{\"bar\": [\\'STRING ONE\\', \\'STRING TWO\\', etc.]}}`\\n- `<object name=\\'baz\\'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{{\\'baz\\': {{\\'foo\\': \\'Some String\\', \\'index\\': 1}}}}`\\n\\n    \\n\\nHuman: \\nAnswer the question below, and return a JSON that follows the correct schema.\\n\\nThis is a test question\\n\\n\\nGiven below is XML that describes the information to extract from this document and the tags to extract it into.\\n\\n\\n    <output>\\n    <string name=\"result\" description=\"chatbot\\'s response\" required=\"true\"/>\\n</output>\\n\\n    \"\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain.llm_chain.prompt.format(context=\"This is a test context\", question=\"This is a test question\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"docs/modules/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=settings.OPENAI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate ConversationalRetrieverChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ConversationalRetrieverChain with Guardrails Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(question, chat_history) -> Dict[str, Any]:\n",
    "    res = qa({\"question\": \"What did the president say about Ketanji Brown Jackson\", \"chat_history\": \"\"})\n",
    "    return output_parser.parse(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run(\n",
    "    question=\"What did the president say about Ketanji Brown Jackson\",\n",
    "    chat_history=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of our nation’s top legal minds and will continue Justice Breyer’s legacy of excellence. She has received a broad range of support since she’s been nominated, from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.'}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
